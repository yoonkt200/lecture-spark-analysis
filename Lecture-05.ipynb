{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce by key, sort 연산 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//--(단어, 1) 튜플 생성....\n",
    "val tupleRDD = filteredRDD.map { x => (x, 1) } // ('spark', 1) 같은 리스트들의 모음이 \n",
    "    \n",
    "//--단어별 개수 카운팅....\n",
    "val reducedRDD = tupleRDD.reduceByKey { (x, y) => x + y} \n",
    "// reduceByKey : key별로 {}안의 연산을 수행함. 'spark' 가 4 line이 있다고 할 때, \n",
    "// {('spark', 'spark') => 1 + 1} 의 결과값을 ('spark', 2) 로 return함. 이를 4 line이 모두 수행되도록 recursive 하게 동작하는 것.\n",
    "// key를 group으로 하는 groupby 연산이라고 생각하면 됨. --> 그룹마다 {}안의 연산을 recursive하게 실행."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort의 두 가지 방법\n",
    "- sortByKey API\n",
    "- top API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// sortByKey API\n",
    "val sortedRDD = swappedRDD.sortByKey(false) //--swappedRDD.sortByKey(false, 1)\n",
    "\n",
    "\n",
    "// top API :\n",
    "val secondValueOrdering = new Ordering[(String, Int)] { //--Ordering 객체 생성....\n",
    "  override def compare(a: (String, Int), b: (String, Int)) = {\n",
    "    a._2 compare b._2  //--(a._2 compare b._2) * -1    //--if((a._2 compare b._2) * -1 == 0) (a._1 compare b._1) * -1 else (a._2 compare b._2) * -1\n",
    "  }\n",
    "}\n",
    "\n",
    "//--개수가 큰 순서로 10개만 로그 출력....\n",
    "reducedRDD.top(10)(secondValueOrdering).foreach { x => println(\"word_count(10 only) : \" + x) }  //--reducedRDD.top(10).foreach { x => println(\"word_count(10 only) : \" + x) }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [LAB] log analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class ApacheAccessLog(ipAddress: String, clientIdentd: String,\n",
    "                                             userId: String, dateTime: String, method: String,\n",
    "                                             endpoint: String, protocol: String,\n",
    "                                             responseCode: Int, contentSize: Long) {\n",
    "\n",
    "}\n",
    "\n",
    "object ApacheAccessLog {\n",
    "  val PATTERN = \"\"\"^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\d+)\"\"\".r\n",
    "\n",
    "  def parseLogLine(log: String): ApacheAccessLog = {\n",
    "    val res = PATTERN.findFirstMatchIn(log)\n",
    "    if (res.isEmpty) {\n",
    "      throw new RuntimeException(\"Cannot parse log line: \" + log)\n",
    "    }\n",
    "    val m = res.get\n",
    "    ApacheAccessLog(m.group(1), m.group(2), m.group(3), m.group(4),\n",
    "      m.group(5), m.group(6), m.group(7), m.group(8).toInt, m.group(9).toLong)\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "/**\n",
    " * The following log statistics will be computed:\n",
    " *   1. The average, min, and max content size of responses returned from the server.\n",
    " *   2. A count of response code's returned. (top count 10)\n",
    " *   3. All IPAddresses that have accessed this server more than N times. (N == 10) (top count 10)\n",
    " *   4. The top endpoints requested by count. (top count 10)\n",
    " * \n",
    " */\n",
    "object LogAnalyzer {\n",
    "  def main(args: Array[String]) {\n",
    "    \n",
    "    lab.common.config.Config.setHadoopHOME\n",
    "    \n",
    "    val logFile = \"src/main/resources/apache.access.log\"\n",
    "    \n",
    "    val sc = new SparkContext(\"local[2]\", \"LogAnalyzer\")  //--local execution (run on eclipse)....\n",
    "    val accessLogs = sc.textFile(logFile).map(ApacheAccessLog.parseLogLine).cache()  //--RDD[ApacheAccessLog].cache()\n",
    "    accessLogs.setName(\"accessLogs\")\n",
    "    //--1. The average, min, and max content size of responses returned from the server.\n",
    "    val contentSizes = accessLogs.map(log => log.contentSize).setName(\"contentSizes\").cache()\n",
    "    println(\"1. Content Size Avg: %s, Min: %s, Max: %s\".format( // reduce, count 등은 모두 각각 action으로 실행됨. \n",
    "                                                                // 그래서 위에서 cache를 사용하면 효율적.\n",
    "                                                                    contentSizes.reduce(_ + _) / contentSizes.count,\n",
    "                                                                    contentSizes.min,\n",
    "                                                                    contentSizes.max))\n",
    "    val stats = contentSizes.stats // stats -> action. 여기서도 action이 실행됨.\n",
    "    println(\"1. Content Size(by StatCounter) Mean: %s, Min: %s, Max: %s\".format( // mean, min등은 실행되는 action이 아님.\n",
    "                                                                                  stats.mean,\n",
    "                                                                                  stats.min,\n",
    "                                                                                  stats.max))\n",
    "\n",
    "    //--2. A count of response code's returned. (top count 10)\n",
    "    val responseCodeToCount = accessLogs.map(log => (log.responseCode, 1))\n",
    "                                                    .reduceByKey(_ + _)\n",
    "                                                    .sortBy(r => r._2, false)\n",
    "                                                    .take(10)\n",
    "    println(s\"\"\"2. Response code counts (top 10): ${responseCodeToCount.mkString(\"[\", \" , \", \"]\")}\"\"\")\n",
    "\n",
    "    //--3. All IPAddresses that have accessed this server more than N times. (N == 10) (top count 10)\n",
    "    val ipAddresses = accessLogs.map(log => (log.ipAddress, 1))\n",
    "                                        .reduceByKey(_ + _)\n",
    "                                        .filter(_._2 > 10)\n",
    "                                        .sortBy(r => r._2, false)\n",
    "                                        //.map(x => x._1)\n",
    "                                        .map(_._1)\n",
    "                                        .take(10)\n",
    "    println(s\"\"\"3. IPAddresses > 10 times (top 10): ${ipAddresses.mkString(\"[\", \" , \", \"]\")}\"\"\")\n",
    "\n",
    "    //--4. The top endpoints requested by count. (top count 10)\n",
    "    val topEndpoints = accessLogs.map(log => (log.endpoint, 1))\n",
    "                                        .reduceByKey(_ + _)\n",
    "                                        .top(10)(OrderingUtils.SecondValueOrdering)\n",
    "    println(s\"\"\"4. Top Endpoints (top 10): ${topEndpoints.mkString(\"[\", \" , \", \"]\")}\"\"\")\n",
    "    \n",
    "    while(true) {}  //--for debug....\n",
    "    sc.stop()\n",
    "  }\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
